# âœ… ìµœì¢… ì•ˆì •í™” ë²„ì „ - GPU(Metal) ë¹„í™œì„±í™” + CPU í•™ìŠµìš© train_model.py

import tensorflow as tf

# âœ… Metal GPU ì™„ì „ ë¹„í™œì„±í™” (Mac M1/M2/M4 ëŒ€ì‘)
tf.config.set_visible_devices([], 'GPU')

import os
import cv2
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from skimage.feature import hog
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Lambda
from tensorflow.keras import backend as K
import time
from tqdm import tqdm

# ----------------------------
# ì„¤ì •
# ----------------------------
image_shape = (128, 128, 1)
handcrafted_dim = 9
csv_path = "/Users/chanyoungko/Desktop/HandWriting/handwriting_pairs_train.csv"
epochs = 10
batch_size = 8  # ì•ˆì •ì  CPU í•™ìŠµ

# ----------------------------
# ìœ í´ë¦¬ë“œ ê±°ë¦¬ í•¨ìˆ˜
# ----------------------------
def euclidean_distance(vects):
    x, y = vects
    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))

# ----------------------------
# ëª¨ë¸ êµ¬ì¡°
# ----------------------------
def build_cnn_branch(input_shape):
    input_img = Input(shape=input_shape)
    x = Conv2D(32, (3, 3), activation='gelu', padding='same')(input_img)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='gelu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Flatten()(x)
    x = Dense(128, activation='gelu')(x)
    return Model(inputs=input_img, outputs=x)

def build_handcrafted_branch(input_dim):
    input_hand = Input(shape=(input_dim,))
    x = Dense(32, activation='gelu')(input_hand)
    return Model(inputs=input_hand, outputs=x)

def build_siamese_model():
    cnn_branch = build_cnn_branch(image_shape)
    hand_branch = build_handcrafted_branch(handcrafted_dim)

    input_img1 = Input(shape=image_shape)
    input_img2 = Input(shape=image_shape)
    input_hand1 = Input(shape=(handcrafted_dim,))
    input_hand2 = Input(shape=(handcrafted_dim,))

    feat_img1 = cnn_branch(input_img1)
    feat_img2 = cnn_branch(input_img2)
    feat_hand1 = hand_branch(input_hand1)
    feat_hand2 = hand_branch(input_hand2)

    merged_feat1 = Concatenate()([feat_img1, feat_hand1])
    merged_feat2 = Concatenate()([feat_img2, feat_hand2])

    embed1 = Dense(64, activation='gelu')(merged_feat1)
    embed2 = Dense(64, activation='gelu')(merged_feat2)

    distance = Lambda(euclidean_distance)([embed1, embed2])
    model = Model(inputs=[input_img1, input_hand1, input_img2, input_hand2], outputs=distance)
    return model

# ----------------------------
# íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
# ----------------------------
def extract_handcrafted_features(img):
    features = []

    # âœ… 1. ì „ì²˜ë¦¬: uint8 + OTSU ì´ì§„í™”
    img_uint8 = img.astype(np.uint8)
    _, binary = cv2.threshold(img_uint8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # âœ… 2. í•„ì•• ê´€ë ¨
    features.append(np.mean(img) / 255.0)  # í‰ê·  ë°ê¸° (ì§„í• ìˆ˜ë¡ ì••ë ¥ ë†’ìŒ)
    features.append(np.std(img) / 255.0)  # í•„ì•• ë³€í™”ëŸ‰

    # âœ… 3. íš ê°œìˆ˜ (ìœ¤ê³½ì„  ìˆ˜)
    edges = cv2.Canny(binary, 50, 150)
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    features.append(len(contours) / 10.0)

    # âœ… 4. ì¢…íš¡ë¹„ (ê¸€ì”¨ ëˆŒë¦¼/ê¸¸ì­‰í•¨)
    h, w = img.shape
    features.append(w / h)

    # âœ… 5. íˆìŠ¤í† ê·¸ë¨ ì§‘ì¤‘ë„ (ë°ê¸° ë¶„í¬ ê· ì¼ì„±)
    hist = cv2.calcHist([img_uint8], [0], None, [32], [0, 256])
    hist = hist / np.sum(hist)
    features.append(np.sum(hist ** 2))

    # âœ… 6~7. ë°©í–¥ì„±: HOG í‰ê·  & í‘œì¤€í¸ì°¨
    hog_descriptor = hog(img_uint8, orientations=8, pixels_per_cell=(16, 16),
                         cells_per_block=(1, 1), visualize=False, feature_vector=True)
    features.append(np.mean(hog_descriptor))
    features.append(np.std(hog_descriptor))

    # âœ… 8. í‰ê·  ê¸°ìš¸ê¸° (ê¸€ì”¨ ë°©í–¥ ì¶”ì •)
    sobelx = cv2.Sobel(img_uint8, cv2.CV_64F, 1, 0, ksize=5)
    sobely = cv2.Sobel(img_uint8, cv2.CV_64F, 0, 1, ksize=5)
    angles = np.arctan2(sobely, sobelx)
    avg_angle = np.mean(angles) / np.pi  # [-Ï€, Ï€] â†’ [-1, 1] ì •ê·œí™”
    features.append(avg_angle)

    # âœ… 9. ëŒ€ë¹„ íŠ¹ì§• (ì•ˆì „í•œ ê³„ì‚° ë°©ì‹)
    pixel_range = np.max(img_uint8) - np.min(img_uint8)
    contrast_feature = pixel_range / (np.mean(img_uint8) + 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    features.append(contrast_feature)

    # âœ… ì •í™•íˆ 9ê°œ ë°˜í™˜, ì•ˆì „í•œ íŠ¹ì§• ì²˜ë¦¬
    features = np.clip(features, -1, 1)  # ëª¨ë“  íŠ¹ì§•ì„ [-1, 1] ë²”ìœ„ë¡œ ì œí•œ
    return np.array(features, dtype=np.float32)


def preprocess_image(path):
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return None
    img = cv2.resize(img, (128, 128))
    return img.astype(np.float32)

# ----------------------------
# í•™ìŠµ í•¨ìˆ˜
# ----------------------------
def load_and_train_model_verbose():
    start_time = time.time()
    df = pd.read_csv(csv_path)

    img1_list, img2_list = [], []
    hand1_list, hand2_list = [], []
    labels = []

    print("ğŸ§ª ì´ë¯¸ì§€ì™€ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ” Loading Data"):
        img1 = preprocess_image(row['image_path_1'])
        img2 = preprocess_image(row['image_path_2'])
        if img1 is None or img2 is None:
            continue
        hand1 = extract_handcrafted_features(img1)
        hand2 = extract_handcrafted_features(img2)
        img1_list.append(np.expand_dims(img1 / 255.0, axis=-1))
        img2_list.append(np.expand_dims(img2 / 255.0, axis=-1))
        hand1_list.append(hand1)
        hand2_list.append(hand2)
        labels.append(row['label'])

    print(f"\nâœ… ì´ {len(labels)}ìŒ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
    data_loading_time = time.time()
    print(f"â±ï¸ ë°ì´í„° ë¡œë”© ì‹œê°„: {data_loading_time - start_time:.2f}ì´ˆ")

    img1_arr = np.array(img1_list)
    img2_arr = np.array(img2_list)
    hand1_arr = np.array(hand1_list)
    hand2_arr = np.array(hand2_list)
    label_arr = np.array(labels).astype(np.float32)

    img1_train, img1_val, img2_train, img2_val, hand1_train, hand1_val, hand2_train, hand2_val, y_train, y_val = train_test_split(
        img1_arr, img2_arr, hand1_arr, hand2_arr, label_arr, test_size=0.2, random_state=42)

    model = build_siamese_model()
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    training_start = time.time()

    history = model.fit(
        [img1_train, hand1_train, img2_train, hand2_train],
        y_train,
        validation_data=([img1_val, hand1_val, img2_val, hand2_val], y_val),
        batch_size=batch_size,
        epochs=epochs,
        verbose=1
    )

    training_end = time.time()
    print(f"\nâœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! â±ï¸ í•™ìŠµ ì‹œê°„: {training_end - training_start:.2f}ì´ˆ")
    print(f"â±ï¸ ì „ì²´ ì†Œìš” ì‹œê°„: {training_end - start_time:.2f}ì´ˆ")

    model.save("siamese_handcrafted_model.h5")
    return history

# ----------------------------
# ë©”ì¸ ì‹¤í–‰
# ----------------------------
if __name__ == "__main__":
    load_and_train_model_verbose()
